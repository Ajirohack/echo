# Fluentd Configuration for Echo Application Logging

# System Configuration
<system>
  log_level info
  suppress_repeated_stacktrace true
  emit_error_log_interval 30s
  suppress_config_dump
  without_source
  workers 2
</system>

# Input Sources

# Kubernetes Container Logs
<source>
  @type tail
  @id kubernetes_logs
  path /var/log/containers/*echo*.log
  pos_file /var/log/fluentd-containers-echo.log.pos
  tag kubernetes.*
  read_from_head true
  <parse>
    @type multi_format
    <pattern>
      format json
      time_key time
      time_format %Y-%m-%dT%H:%M:%S.%NZ
    </pattern>
    <pattern>
      format /^(?<time>.+) (?<stream>stdout|stderr) [^ ]* (?<log>.*)$/
      time_format %Y-%m-%dT%H:%M:%S.%N%:z
    </pattern>
  </parse>
</source>

# Application Logs via HTTP
<source>
  @type http
  @id http_input
  port 9880
  bind 0.0.0.0
  body_size_limit 32m
  keepalive_timeout 10s
  <parse>
    @type json
  </parse>
  <transport tls>
    cert_path /etc/ssl/certs/fluentd.crt
    private_key_path /etc/ssl/private/fluentd.key
  </transport>
</source>

# Forward Input for other Fluentd instances
<source>
  @type forward
  @id forward_input
  port 24224
  bind 0.0.0.0
</source>

# Prometheus Metrics
<source>
  @type prometheus
  bind 0.0.0.0
  port 24231
  metrics_path /metrics
</source>

<source>
  @type prometheus_output_monitor
  interval 10
  <labels>
    hostname ${hostname}
  </labels>
</source>

# Filters

# Kubernetes Metadata Enhancement
<filter kubernetes.**>
  @type kubernetes_metadata
  @id kubernetes_metadata
  kubernetes_url "#{ENV['KUBERNETES_SERVICE_HOST']}:#{ENV['KUBERNETES_SERVICE_PORT_HTTPS']}"
  verify_ssl true
  ca_file /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
  bearer_token_file /var/run/secrets/kubernetes.io/serviceaccount/token
  cache_size 1000
  cache_ttl 3600
  watch true
  de_dot false
  annotation_match [ ".*" ]
  allow_orphans true
</filter>

# Echo Application Log Processing
<filter kubernetes.var.log.containers.**echo-app**>
  @type parser
  @id echo_app_parser
  key_name log
  reserve_data true
  remove_key_name_field true
  emit_invalid_record_to_error false
  <parse>
    @type multi_format
    <pattern>
      format json
      time_key timestamp
      time_format %Y-%m-%dT%H:%M:%S.%LZ
    </pattern>
    <pattern>
      format /^\[(?<timestamp>\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}\.\d{3}Z)\] (?<level>\w+): (?<message>.*)$/
      time_key timestamp
      time_format %Y-%m-%dT%H:%M:%S.%LZ
    </pattern>
    <pattern>
      format none
    </pattern>
  </parse>
</filter>

# Add Environment and Service Information
<filter kubernetes.var.log.containers.**echo**>
  @type record_transformer
  @id echo_record_transformer
  enable_ruby true
  auto_typecast true
  <record>
    service_name "${record['kubernetes']['labels']['app'] || 'unknown'}"
    environment "${record['kubernetes']['namespace_name']}"
    cluster_name "#{ENV['CLUSTER_NAME'] || 'echo-cluster'}"
    node_name "${record['kubernetes']['host']}"
    pod_name "${record['kubernetes']['pod_name']}"
    container_name "${record['kubernetes']['container_name']}"
    log_source "kubernetes"
    @timestamp "${time.strftime('%Y-%m-%dT%H:%M:%S.%LZ')}"
  </record>
</filter>

# Security and Audit Log Processing
<filter kubernetes.var.log.containers.**echo** security.**>
  @type grep
  @id security_filter
  <regexp>
    key message
    pattern /(authentication|authorization|security|audit|login|logout|access_denied|suspicious)/i
  </regexp>
</filter>

<filter security.**>
  @type record_transformer
  @id security_transformer
  <record>
    log_type "security"
    severity "${record['level'] == 'ERROR' ? 'high' : record['level'] == 'WARN' ? 'medium' : 'low'}"
  </record>
</filter>

# Error Log Processing
<filter kubernetes.var.log.containers.**echo**>
  @type grep
  @id error_filter
  <regexp>
    key level
    pattern /^(ERROR|FATAL)$/i
  </regexp>
</filter>

# Performance Log Processing
<filter kubernetes.var.log.containers.**echo**>
  @type parser
  @id performance_parser
  key_name message
  reserve_data true
  emit_invalid_record_to_error false
  <parse>
    @type regexp
    expression /^Performance: (?<operation>\w+) took (?<duration>\d+(?:\.\d+)?)ms(?: \| (?<additional_info>.*))?$/
    types duration:float
  </parse>
</filter>

# WebRTC Log Processing
<filter kubernetes.var.log.containers.**echo**>
  @type parser
  @id webrtc_parser
  key_name message
  reserve_data true
  emit_invalid_record_to_error false
  <parse>
    @type regexp
    expression /^WebRTC: (?<event>\w+) \| Connection: (?<connection_id>[\w-]+) \| (?<details>.*)$/
  </parse>
</filter>

# Audio Processing Log Processing
<filter kubernetes.var.log.containers.**echo**>
  @type parser
  @id audio_parser
  key_name message
  reserve_data true
  emit_invalid_record_to_error false
  <parse>
    @type regexp
    expression /^Audio: (?<operation>\w+) \| Quality: (?<quality>\d+(?:\.\d+)?) \| Latency: (?<latency>\d+(?:\.\d+)?)ms(?: \| (?<additional_info>.*))?$/
    types quality:float,latency:float
  </parse>
</filter>

# Rate Limiting for High Volume Logs
<filter kubernetes.var.log.containers.**>
  @type sampling
  @id sampling_filter
  sampling_rate 10
  <regexp>
    key level
    pattern /^DEBUG$/i
  </regexp>
</filter>

# Output Configurations

# Elasticsearch for General Logs
<match kubernetes.var.log.containers.**echo**>
  @type elasticsearch
  @id elasticsearch_echo
  host "#{ENV['ELASTICSEARCH_HOST'] || 'elasticsearch'}"
  port "#{ENV['ELASTICSEARCH_PORT'] || '9200'}"
  scheme "#{ENV['ELASTICSEARCH_SCHEME'] || 'http'}"
  user "#{ENV['ELASTICSEARCH_USER']}"
  password "#{ENV['ELASTICSEARCH_PASSWORD']}"
  
  # Index Configuration
  index_name "echo-logs"
  type_name "_doc"
  
  # Template Configuration
  template_name "echo-logs"
  template_file "/etc/fluentd/templates/echo-logs.json"
  template_overwrite true
  
  # ILM Configuration
  ilm_policy_id "echo-logs-policy"
  ilm_policy_overwrite true
  ilm_policy {
    "policy": {
      "phases": {
        "hot": {
          "actions": {
            "rollover": {
              "max_size": "10GB",
              "max_age": "7d"
            }
          }
        },
        "warm": {
          "min_age": "7d",
          "actions": {
            "allocate": {
              "number_of_replicas": 0
            }
          }
        },
        "cold": {
          "min_age": "30d",
          "actions": {
            "allocate": {
              "number_of_replicas": 0
            }
          }
        },
        "delete": {
          "min_age": "90d"
        }
      }
    }
  }
  
  # Buffer Configuration
  <buffer>
    @type file
    path /var/log/fluentd-buffers/elasticsearch-echo
    flush_mode interval
    retry_type exponential_backoff
    flush_thread_count 2
    flush_interval 5s
    retry_forever
    retry_max_interval 30
    chunk_limit_size 2M
    queue_limit_length 8
    overflow_action block
  </buffer>
  
  # Error Handling
  <secondary>
    @type file
    path /var/log/fluentd-failed-records/elasticsearch-echo
    <buffer>
      flush_mode immediate
    </buffer>
  </secondary>
end

# Security Logs to Dedicated Index
<match security.**>
  @type elasticsearch
  @id elasticsearch_security
  host "#{ENV['ELASTICSEARCH_HOST'] || 'elasticsearch'}"
  port "#{ENV['ELASTICSEARCH_PORT'] || '9200'}"
  scheme "#{ENV['ELASTICSEARCH_SCHEME'] || 'http'}"
  user "#{ENV['ELASTICSEARCH_USER']}"
  password "#{ENV['ELASTICSEARCH_PASSWORD']}"
  
  index_name "echo-security-logs"
  type_name "_doc"
  
  <buffer>
    @type file
    path /var/log/fluentd-buffers/elasticsearch-security
    flush_mode immediate
    retry_type exponential_backoff
    flush_thread_count 1
    retry_forever
    retry_max_interval 30
    chunk_limit_size 1M
    queue_limit_length 4
  </buffer>
end

# Performance Metrics to InfluxDB
<match performance.**>
  @type influxdb
  @id influxdb_performance
  host "#{ENV['INFLUXDB_HOST'] || 'influxdb'}"
  port "#{ENV['INFLUXDB_PORT'] || '8086'}"
  dbname "#{ENV['INFLUXDB_DATABASE'] || 'echo_performance'}"
  user "#{ENV['INFLUXDB_USER']}"
  password "#{ENV['INFLUXDB_PASSWORD']}"
  
  measurement "performance_metrics"
  tag_keys ["service_name", "environment", "operation"]
  field_keys ["duration"]
  
  <buffer>
    @type memory
    flush_mode interval
    flush_interval 10s
    chunk_limit_size 1M
  </buffer>
end

# Error Logs to Slack (Critical Errors Only)
<match kubernetes.var.log.containers.**echo** error.**>
  @type slack
  @id slack_errors
  webhook_url "#{ENV['SLACK_WEBHOOK_URL']}"
  channel "#echo-alerts"
  username "Echo-Logger"
  icon_emoji ":warning:"
  
  title "Critical Error in Echo Application"
  message "Environment: %s\nService: %s\nPod: %s\nError: %s"
  message_keys environment,service_name,pod_name,message
  
  <buffer>
    @type memory
    flush_mode immediate
    retry_type exponential_backoff
    retry_forever false
    retry_max_times 3
  </buffer>
end

# Audit Logs to File (Compliance)
<match security.** audit.**>
  @type file
  @id file_audit
  path "/var/log/echo-audit/audit-%Y%m%d.log"
  append true
  
  <format>
    @type json
  </format>
  
  <buffer time>
    timekey 1d
    timekey_wait 10m
    timekey_use_utc true
    path /var/log/fluentd-buffers/audit
  </buffer>
end

# Metrics Export to Prometheus
<match prometheus.**>
  @type prometheus
  @id prometheus_metrics
  
  <metric>
    name fluentd_input_status_num_records_total
    type counter
    desc The total number of incoming records
    <labels>
      tag ${tag}
      hostname ${hostname}
    </labels>
  </metric>
  
  <metric>
    name fluentd_output_status_num_records_total
    type counter
    desc The total number of outgoing records
    <labels>
      tag ${tag}
      hostname ${hostname}
    </labels>
  </metric>
end

# Catch-all for unmatched logs
<match **>
  @type file
  @id file_catchall
  path "/var/log/echo-unmatched/unmatched-%Y%m%d.log"
  append true
  
  <format>
    @type json
  </format>
  
  <buffer time>
    timekey 1d
    timekey_wait 10m
    path /var/log/fluentd-buffers/unmatched
  </buffer>
end

# Health Check Endpoint
<source>
  @type http
  @id health_check
  port 9881
  bind 0.0.0.0
  <parse>
    @type none
  </parse>
</source>

<match health.check>
  @type null
end